{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKIyLghs0MCVI1tmfYrmqo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sunday-Okey/NLP_Project_Machine_Translation/blob/master/NLP_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "bfTRoQDEmR_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "if IS_COLAB:\n",
        "    %pip install -q -U tensorflow-addons\n",
        "    %pip install -q -U transformers\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "    if IS_KAGGLE:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")"
      ],
      "metadata": {
        "id": "1z3AZZJol2vO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras_preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mZI4EGI5LMp",
        "outputId": "e1758d75-9bc6-459a-a37b-e0a2e6210b81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras_preprocessing in /usr/local/lib/python3.8/dist-packages (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras_preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from keras_preprocessing) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "IjeD0n_83Xd0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data\n",
        "import urllib.request\n",
        "datapath = os.path.join(\"data\", \"\")\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/Sunday-Okey/NLP_Project_Machine_Translation/master\"\n",
        "os.makedirs(datapath, exist_ok=True)\n",
        "for filename in (\"small_vocab_en\", \"small_vocab_fr\"):\n",
        "    print(\"Downloading\", filename)\n",
        "    url = DOWNLOAD_ROOT + \"/data/\" + filename\n",
        "    urllib.request.urlretrieve(url, datapath + filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoM3yCgN2TFq",
        "outputId": "49542878-d094-424d-ab5a-274706be48d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading small_vocab_en\n",
            "Downloading small_vocab_fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download project files\n",
        "import urllib.request\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/Sunday-Okey/NLP_Project_Machine_Translation/master/\"\n",
        "for filename in (\"helper.py\", \"project_tests.py\"):\n",
        "    print(\"Downloading\", filename)\n",
        "    url = DOWNLOAD_ROOT + filename\n",
        "    urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN-VHlov3xn8",
        "outputId": "02028aa4-f29c-4ca2-fb1d-a64a510364c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading helper.py\n",
            "Downloading project_tests.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZxNpiCTqic3i"
      },
      "outputs": [],
      "source": [
        "# import the libraries\n",
        "import collections\n",
        "import helper\n",
        "import numpy as np\n",
        "import project_tests as tests\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English data\n",
        "english_sentences = helper.load_data('data/small_vocab_en')\n",
        "# Load French data\n",
        "french_sentences = helper.load_data('data/small_vocab_fr')\n",
        "\n",
        "print('Dataset Loaded')\n",
        "print(type(english_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CEYsG2dleOD",
        "outputId": "113cacfd-17de-430d-87dc-7214d9c125e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by investigating the dataset that will be used to train and evaluate our pipeline"
      ],
      "metadata": {
        "id": "89SxEF_aoIKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Files\n",
        "\n",
        "Each line in small_vocab_en contains an English sentence with the respective translation in each line of small_vocab_fr. View the first two lines from each file."
      ],
      "metadata": {
        "id": "wztnuX8MoZvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in range(2):\n",
        "    print(f'small_vocab_en Line {sample + 1}:  {english_sentences[sample]}')\n",
        "    print(f'small_vocab_fr Line {sample + 1}:  {english_sentences[sample]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Asok5ExZoYQS",
        "outputId": "a48a73ea-2707-44c3-c390-55042e5cfeae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "small_vocab_fr Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
            "small_vocab_fr Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From looking at the sentences, we can see they have been preprocessed already. The puncuations have been replaced by spaces. All the text have been converted to lowercase."
      ],
      "metadata": {
        "id": "yin66vxHpXz1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "19kOY85I5hi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oVUaTb0Cmip4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRaKJ3oQ5hHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D9ncNJnJoL77"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}